{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364ab44e-1f75-4c9c-9f5b-3669411d9e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7f5324-c8f6-48e6-a935-3f895817f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12dc96c-cb21-4eec-b7c9-bcedc44437ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (2.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28fd95a-1594-4689-bfc1-6a9acb95a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (1.17.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (12.0.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (6.33.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhushan paunikar\\appdata\\roaming\\python\\python313\\site-packages (from colorlog->optuna) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7131e151-a376-441f-8fa9-b79982a914cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bhushan paunikar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fe046d5-7dd5-49bf-b9c2-cecb86b4d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, DataCollatorWithPadding\n",
    "\n",
    "import pandas as pd # For loading your CSV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import logging\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configure logging and TensorBoard\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# --- GLOBAL CONSTANTS ---\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "CHECKPOINT_DIR = \"model_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define hyperparameters (Optuna will overwrite these later)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# CRITICAL FIX: Custom Labels for 3-Class Problem\n",
    "NUM_LABELS = 3\n",
    "ID2LABEL = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "LABEL2ID = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "# From your original code's global constants section\n",
    "DATA_FILE_PATH = 'cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f68ffa2e-318e-43d2-b7fe-9d67e66356fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for saving checkpoints\n",
    "CHECKPOINT_DIR = \"model_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d8d2a90-7a66-42a8-864a-012d12a0fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning using Optuna\n",
    "def objective(trial):\n",
    "    # 1. Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
    "\n",
    "    # Note: Model loading must be updated globally (num_labels=3, ignore_mismatched_sizes=True)\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize the data collator to handle padding at the batch level\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # CRITICAL FIX: 3-WAY DATA SPLITTING AND CUSTOM DATA LOADING\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_full = pd.read_csv(DATA_FILE_PATH)\n",
    "    # Data Cleaning and Mapping\n",
    "    columns_to_drop = ['Persona', 'Specific Clause Referenced', 'Unnamed: 4', 'Unnamed: 5']\n",
    "    df_clean = df_full.drop(columns=columns_to_drop, errors='ignore')\n",
    "    df_clean.rename(columns={'comment': 'text'}, inplace=True)\n",
    "    df_clean['label'] = df_clean['label'].map(LABEL2ID)\n",
    "\n",
    "    # A. Split into Main (80%) and Test (20%)\n",
    "    df_main, _ = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['label']) \n",
    "    \n",
    "    # B. Split Main (80%) into Train (70% total) and Validation (10% total)\n",
    "    df_train, df_val = train_test_split(df_main, test_size=1/8, random_state=42, stratify=df_main['label'])\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train).remove_columns(['__index_level_0__'])\n",
    "    val_dataset = Dataset.from_pandas(df_val).remove_columns(['__index_level_0__'])\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # 3. Training Data Loaders\n",
    "    train_shuffled = train_dataset.shuffle(seed=42) \n",
    "    train_dataset_tokenized = preprocess_data(tokenizer, train_shuffled) \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_tokenized, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=data_collator # CRITICAL FIX: Use DataCollator\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # 5. Train the model (pass the current trial number for status update)\n",
    "    train_model(model, train_loader, optimizer, lr_scheduler, device, trial.number)\n",
    "\n",
    "    # 6. Evaluate on the DEDICATED VALIDATION SET\n",
    "    val_shuffled = val_dataset.shuffle(seed=42)\n",
    "    val_dataset_tokenized = preprocess_data(tokenizer, val_shuffled) \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_tokenized, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=data_collator # CRITICAL FIX: Use DataCollator\n",
    "    )\n",
    "\n",
    "    predictions, labels = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    # CRITICAL FIX: Use 'weighted' F1 score (multi-class)\n",
    "    validation_f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Explicit status output after trial completes\n",
    "    logging.info(\"----------------------------------------------------------\")\n",
    "    logging.info(f\"TRIAL {trial.number} SUMMARY: SUCCESS\")\n",
    "    logging.info(f\"  Learning Rate: {trial.params['learning_rate']:.7f}\")\n",
    "    logging.info(f\"  Batch Size: {trial.params['batch_size']}\")\n",
    "    logging.info(f\"  Validation F1-Score: {validation_f1:.4f}\")\n",
    "    logging.info(\"----------------------------------------------------------\")\n",
    "    \n",
    "    return validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f3ae1d0-9dd5-456c-890b-841cbe6e287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these global constants are defined:\n",
    "# NUM_LABELS = 3\n",
    "# ID2LABEL = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "# LABEL2ID = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # CRITICAL FIX: Add multi-class configuration and size mismatch override\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=NUM_LABELS,   # Tells the model to use 3 output neurons\n",
    "        id2label=ID2LABEL,       # Maps prediction IDs (0, 1, 2) to labels\n",
    "        label2id=LABEL2ID,        # Maps labels to IDs\n",
    "        ignore_mismatched_sizes=True # Fixes the conflict between old 2-class head and new 3-class head\n",
    "    )\n",
    "\n",
    "    # LoRA configuration remains the same\n",
    "    target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    logging.info(\"LoRA model initialized.\")\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b8aced6-b399-42cd-8252-4485e1e711ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(tokenizer, dataset):\n",
    "    def tokenize(batch):\n",
    "        # CRITICAL FIX: Removed padding=True. \n",
    "        # Padding is now handled by the DataCollatorWithPadding in the DataLoader.\n",
    "        return tokenizer(batch['text'], truncation=True, max_length=128)\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    # The format setting is correct for the Dataset object\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c52875ec-a162-4273-aec2-f045fcf97b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, lr_scheduler, device, trial_identifier):\n",
    "    model.train()\n",
    "    \n",
    "    total_epoch_loss = 0.0\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        logging.info(f\"Starting epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Reset tracking variables for each epoch\n",
    "        total_epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # CRITICAL FIX 1: Use 'labels' key (plural) from DataCollator\n",
    "                labels = batch['labels'].to(device) \n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Accumulate loss and batch count for robust averaging\n",
    "                total_epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # CRITICAL FIX 3: Add live status update for continuous feedback\n",
    "                if i % 50 == 0: \n",
    "                    print(f\"-> Trial {trial_identifier}, Epoch {epoch+1}/{NUM_EPOCHS}, Batch {i}/{len(train_loader)}. Current Loss: {loss.item():.4f}\", end='\\r')\n",
    "                \n",
    "                writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during training: {e}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        # Added \\n to move the cursor past the live status update before logging\n",
    "        logging.info(f\"\\nCheckpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        # CRITICAL FIX 2: Calculate and log the average loss for the epoch\n",
    "        if num_batches > 0:\n",
    "            avg_loss = total_epoch_loss / num_batches\n",
    "            logging.info(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            logging.info(f\"Epoch {epoch + 1} completed. No batches processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd4c70d1-c906-4536-ae38-67dc4bd97dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # CRITICAL FIX: Change 'label' to 'labels' to match DataCollator output\n",
    "                labels.extend(batch['labels'].tolist())\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation: {e}\") \n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cbbea30-9f7c-434b-92e8-34c32244374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify text\n",
    "def classify_text(model, tokenizer, text, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "            probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during text classification: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    return predicted_class, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa03fe8c-7e93-4c7b-a71a-5a4fd7392a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 00:26:11,897 - INFO - LoRA model initialized.\n",
      "2025-11-08 00:26:11,908 - INFO - Preparing final Train/Test data sets...\n",
      "[I 2025-11-08 00:26:11,985] A new study created in memory with name: no-name-12a67651-16ff-466d-a427-6c5f3a01ad64\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 00:26:13,340 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 37780.03 examples/s]\n",
      "2025-11-08 00:26:13,731 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 0, Epoch 1/3, Batch 700/731. Current Loss: 0.8217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 00:33:58,460 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 00:33:58,461 - INFO - Epoch 1 completed. Average Loss: 0.8098\n",
      "2025-11-08 00:33:58,462 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 0, Epoch 2/3, Batch 700/731. Current Loss: 0.5758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 00:41:54,938 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 00:41:54,939 - INFO - Epoch 2 completed. Average Loss: 0.6898\n",
      "2025-11-08 00:41:54,940 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 0, Epoch 3/3, Batch 700/731. Current Loss: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 00:49:52,177 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 00:49:52,178 - INFO - Epoch 3 completed. Average Loss: 0.6613\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 36153.13 examples/s]\n",
      "2025-11-08 00:50:12,983 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 00:50:12,984 - INFO - TRIAL 0 SUMMARY: SUCCESS\n",
      "2025-11-08 00:50:12,984 - INFO -   Learning Rate: 0.0000373\n",
      "2025-11-08 00:50:12,985 - INFO -   Batch Size: 16\n",
      "2025-11-08 00:50:12,985 - INFO -   Validation F1-Score: 0.7256\n",
      "2025-11-08 00:50:12,986 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 00:50:13,047] Trial 0 finished with value: 0.7256319471440527 and parameters: {'learning_rate': 3.734981164155812e-05, 'batch_size': 16}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 00:50:14,676 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 36548.79 examples/s]\n",
      "2025-11-08 00:50:15,085 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 1, Epoch 1/3, Batch 350/366. Current Loss: 0.8311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 00:58:56,037 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 00:58:56,038 - INFO - Epoch 1 completed. Average Loss: 0.9218\n",
      "2025-11-08 00:58:56,038 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 1, Epoch 2/3, Batch 350/366. Current Loss: 0.6989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 01:07:34,745 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 01:07:34,746 - INFO - Epoch 2 completed. Average Loss: 0.8505\n",
      "2025-11-08 01:07:34,747 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 1, Epoch 3/3, Batch 350/366. Current Loss: 0.8472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 01:17:12,194 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 01:17:12,196 - INFO - Epoch 3 completed. Average Loss: 0.8251\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 11835.49 examples/s]\n",
      "2025-11-08 01:17:45,887 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 01:17:45,889 - INFO - TRIAL 1 SUMMARY: SUCCESS\n",
      "2025-11-08 01:17:45,891 - INFO -   Learning Rate: 0.0000104\n",
      "2025-11-08 01:17:45,892 - INFO -   Batch Size: 32\n",
      "2025-11-08 01:17:45,893 - INFO -   Validation F1-Score: 0.6444\n",
      "2025-11-08 01:17:45,894 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 01:17:45,917] Trial 1 finished with value: 0.644420122171886 and parameters: {'learning_rate': 1.0405851437859949e-05, 'batch_size': 32}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 01:17:48,368 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 18403.59 examples/s]\n",
      "2025-11-08 01:17:49,200 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 2, Epoch 1/3, Batch 700/731. Current Loss: 0.8203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 01:31:38,721 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 01:31:38,722 - INFO - Epoch 1 completed. Average Loss: 0.9066\n",
      "2025-11-08 01:31:38,724 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 2, Epoch 2/3, Batch 700/731. Current Loss: 1.0694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 01:45:35,763 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 01:45:35,764 - INFO - Epoch 2 completed. Average Loss: 0.8137\n",
      "2025-11-08 01:45:35,766 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 2, Epoch 3/3, Batch 700/731. Current Loss: 0.6367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 01:59:27,508 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 01:59:27,510 - INFO - Epoch 3 completed. Average Loss: 0.7847\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 14011.96 examples/s]\n",
      "2025-11-08 02:00:01,243 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 02:00:01,245 - INFO - TRIAL 2 SUMMARY: SUCCESS\n",
      "2025-11-08 02:00:01,246 - INFO -   Learning Rate: 0.0000113\n",
      "2025-11-08 02:00:01,247 - INFO -   Batch Size: 16\n",
      "2025-11-08 02:00:01,248 - INFO -   Validation F1-Score: 0.6746\n",
      "2025-11-08 02:00:01,249 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 02:00:01,354] Trial 2 finished with value: 0.6745573419332134 and parameters: {'learning_rate': 1.1318201406569842e-05, 'batch_size': 16}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 02:00:03,751 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 16216.97 examples/s]\n",
      "2025-11-08 02:00:04,675 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 3, Epoch 1/3, Batch 1450/1462. Current Loss: 0.9094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:13:01,325 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 02:13:01,327 - INFO - Epoch 1 completed. Average Loss: 0.8609\n",
      "2025-11-08 02:13:01,328 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 3, Epoch 2/3, Batch 1450/1462. Current Loss: 0.4151"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:25:53,300 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 02:25:53,302 - INFO - Epoch 2 completed. Average Loss: 0.7589\n",
      "2025-11-08 02:25:53,303 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 3, Epoch 3/3, Batch 1450/1462. Current Loss: 0.5776"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:38:44,227 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 02:38:44,229 - INFO - Epoch 3 completed. Average Loss: 0.7256\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 12853.53 examples/s]\n",
      "2025-11-08 02:39:14,687 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 02:39:14,688 - INFO - TRIAL 3 SUMMARY: SUCCESS\n",
      "2025-11-08 02:39:14,689 - INFO -   Learning Rate: 0.0000134\n",
      "2025-11-08 02:39:14,690 - INFO -   Batch Size: 8\n",
      "2025-11-08 02:39:14,691 - INFO -   Validation F1-Score: 0.7005\n",
      "2025-11-08 02:39:14,692 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 02:39:14,797] Trial 3 finished with value: 0.7005313314712159 and parameters: {'learning_rate': 1.343276693561499e-05, 'batch_size': 8}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 02:39:16,934 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 15217.82 examples/s]\n",
      "2025-11-08 02:39:17,893 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 4, Epoch 1/3, Batch 350/366. Current Loss: 0.7886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:53:38,630 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 02:53:38,631 - INFO - Epoch 1 completed. Average Loss: 0.8695\n",
      "2025-11-08 02:53:38,632 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 4, Epoch 2/3, Batch 350/366. Current Loss: 0.5944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:02:21,610 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 03:02:21,611 - INFO - Epoch 2 completed. Average Loss: 0.7670\n",
      "2025-11-08 03:02:21,612 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 4, Epoch 3/3, Batch 350/366. Current Loss: 0.8454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:11:02,305 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 03:11:02,306 - INFO - Epoch 3 completed. Average Loss: 0.7297\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 39658.91 examples/s]\n",
      "2025-11-08 03:11:24,632 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 03:11:24,633 - INFO - TRIAL 4 SUMMARY: SUCCESS\n",
      "2025-11-08 03:11:24,634 - INFO -   Learning Rate: 0.0000274\n",
      "2025-11-08 03:11:24,634 - INFO -   Batch Size: 32\n",
      "2025-11-08 03:11:24,635 - INFO -   Validation F1-Score: 0.6987\n",
      "2025-11-08 03:11:24,635 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 03:11:24,698] Trial 4 finished with value: 0.6986821348355473 and parameters: {'learning_rate': 2.7403242761307118e-05, 'batch_size': 32}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 03:11:26,418 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 36532.07 examples/s]\n",
      "2025-11-08 03:11:26,824 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 5, Epoch 1/3, Batch 700/731. Current Loss: 0.9129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:19:17,996 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 03:19:17,997 - INFO - Epoch 1 completed. Average Loss: 1.0070\n",
      "2025-11-08 03:19:17,997 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 5, Epoch 2/3, Batch 700/731. Current Loss: 0.9572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:27:07,979 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 03:27:07,980 - INFO - Epoch 2 completed. Average Loss: 0.9228\n",
      "2025-11-08 03:27:07,981 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 5, Epoch 3/3, Batch 700/731. Current Loss: 0.9622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:34:54,280 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 03:34:54,281 - INFO - Epoch 3 completed. Average Loss: 0.9067\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 40749.34 examples/s]\n",
      "2025-11-08 03:35:14,338 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 03:35:14,339 - INFO - TRIAL 5 SUMMARY: SUCCESS\n",
      "2025-11-08 03:35:14,339 - INFO -   Learning Rate: 0.0000021\n",
      "2025-11-08 03:35:14,340 - INFO -   Batch Size: 16\n",
      "2025-11-08 03:35:14,341 - INFO -   Validation F1-Score: 0.6013\n",
      "2025-11-08 03:35:14,341 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 03:35:14,398] Trial 5 finished with value: 0.6013302134780154 and parameters: {'learning_rate': 2.1079192169158175e-06, 'batch_size': 16}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 03:35:15,997 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 36863.84 examples/s]\n",
      "2025-11-08 03:35:16,394 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 6, Epoch 1/3, Batch 450/488. Current Loss: 0.9290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:43:37,538 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 03:43:37,539 - INFO - Epoch 1 completed. Average Loss: 0.9917\n",
      "2025-11-08 03:43:37,540 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 6, Epoch 2/3, Batch 450/488. Current Loss: 0.9620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 03:51:54,682 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 03:51:54,683 - INFO - Epoch 2 completed. Average Loss: 0.9264\n",
      "2025-11-08 03:51:54,683 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 6, Epoch 3/3, Batch 450/488. Current Loss: 0.8902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:00:14,945 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 04:00:14,946 - INFO - Epoch 3 completed. Average Loss: 0.9127\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 41480.57 examples/s]\n",
      "2025-11-08 04:00:36,761 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 04:00:36,762 - INFO - TRIAL 6 SUMMARY: SUCCESS\n",
      "2025-11-08 04:00:36,762 - INFO -   Learning Rate: 0.0000023\n",
      "2025-11-08 04:00:36,763 - INFO -   Batch Size: 24\n",
      "2025-11-08 04:00:36,763 - INFO -   Validation F1-Score: 0.5925\n",
      "2025-11-08 04:00:36,764 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 04:00:36,775] Trial 6 finished with value: 0.5924542801382369 and parameters: {'learning_rate': 2.3245639589053203e-06, 'batch_size': 24}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 04:00:38,555 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 16332.12 examples/s]\n",
      "2025-11-08 04:00:39,345 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 7, Epoch 1/3, Batch 450/488. Current Loss: 0.8596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:08:57,983 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 04:08:57,984 - INFO - Epoch 1 completed. Average Loss: 0.9876\n",
      "2025-11-08 04:08:57,985 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 7, Epoch 2/3, Batch 450/488. Current Loss: 0.8507"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:17:17,726 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 04:17:17,727 - INFO - Epoch 2 completed. Average Loss: 0.9050\n",
      "2025-11-08 04:17:17,727 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 7, Epoch 3/3, Batch 450/488. Current Loss: 1.0263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:25:35,724 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 04:25:35,725 - INFO - Epoch 3 completed. Average Loss: 0.8895\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 28461.88 examples/s]\n",
      "2025-11-08 04:25:57,850 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 04:25:57,851 - INFO - TRIAL 7 SUMMARY: SUCCESS\n",
      "2025-11-08 04:25:57,851 - INFO -   Learning Rate: 0.0000040\n",
      "2025-11-08 04:25:57,852 - INFO -   Batch Size: 24\n",
      "2025-11-08 04:25:57,852 - INFO -   Validation F1-Score: 0.6113\n",
      "2025-11-08 04:25:57,853 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 04:25:57,911] Trial 7 finished with value: 0.6113456785669089 and parameters: {'learning_rate': 3.972267319714778e-06, 'batch_size': 24}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 04:25:59,666 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 35157.80 examples/s]\n",
      "2025-11-08 04:26:00,078 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 8, Epoch 1/3, Batch 1450/1462. Current Loss: 0.9020"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:33:21,997 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 04:33:21,998 - INFO - Epoch 1 completed. Average Loss: 0.9253\n",
      "2025-11-08 04:33:21,999 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 8, Epoch 2/3, Batch 1450/1462. Current Loss: 0.9046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:40:43,980 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 04:40:43,982 - INFO - Epoch 2 completed. Average Loss: 0.8499\n",
      "2025-11-08 04:40:43,982 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 8, Epoch 3/3, Batch 1450/1462. Current Loss: 0.9021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:48:07,482 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 04:48:07,484 - INFO - Epoch 3 completed. Average Loss: 0.8308\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 38604.27 examples/s]\n",
      "2025-11-08 04:48:27,030 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 04:48:27,031 - INFO - TRIAL 8 SUMMARY: SUCCESS\n",
      "2025-11-08 04:48:27,032 - INFO -   Learning Rate: 0.0000053\n",
      "2025-11-08 04:48:27,032 - INFO -   Batch Size: 8\n",
      "2025-11-08 04:48:27,033 - INFO -   Validation F1-Score: 0.6490\n",
      "2025-11-08 04:48:27,033 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 04:48:27,077] Trial 8 finished with value: 0.6489938234168177 and parameters: {'learning_rate': 5.266263481198898e-06, 'batch_size': 8}. Best is trial 0 with value: 0.7256319471440527.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-08 04:48:29,122 - INFO - LoRA model initialized.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 37779.65 examples/s]\n",
      "2025-11-08 04:48:29,507 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 9, Epoch 1/3, Batch 350/366. Current Loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 04:56:54,033 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 04:56:54,033 - INFO - Epoch 1 completed. Average Loss: 0.8089\n",
      "2025-11-08 04:56:54,034 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 9, Epoch 2/3, Batch 350/366. Current Loss: 0.6574"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:05:19,501 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 05:05:19,501 - INFO - Epoch 2 completed. Average Loss: 0.6804\n",
      "2025-11-08 05:05:19,502 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial 9, Epoch 3/3, Batch 350/366. Current Loss: 0.7492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:13:46,655 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 05:13:46,655 - INFO - Epoch 3 completed. Average Loss: 0.6493\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1671/1671 [00:00<00:00, 41841.13 examples/s]\n",
      "2025-11-08 05:14:08,553 - INFO - ----------------------------------------------------------\n",
      "2025-11-08 05:14:08,554 - INFO - TRIAL 9 SUMMARY: SUCCESS\n",
      "2025-11-08 05:14:08,554 - INFO -   Learning Rate: 0.0000632\n",
      "2025-11-08 05:14:08,555 - INFO -   Batch Size: 32\n",
      "2025-11-08 05:14:08,556 - INFO -   Validation F1-Score: 0.7319\n",
      "2025-11-08 05:14:08,556 - INFO - ----------------------------------------------------------\n",
      "[I 2025-11-08 05:14:08,617] Trial 9 finished with value: 0.7319323551184633 and parameters: {'learning_rate': 6.323547064573412e-05, 'batch_size': 32}. Best is trial 9 with value: 0.7319323551184633.\n",
      "2025-11-08 05:14:08,618 - INFO - Best hyperparameters:\n",
      "2025-11-08 05:14:08,618 - INFO - {'learning_rate': 6.323547064573412e-05, 'batch_size': 32}\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 11690/11690 [00:00<00:00, 34846.36 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 3341/3341 [00:00<00:00, 38254.42 examples/s]\n",
      "2025-11-08 05:14:09,066 - INFO - Starting final fine-tuning of the model.\n",
      "2025-11-08 05:14:09,067 - INFO - Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial Final, Epoch 1/3, Batch 350/366. Current Loss: 0.7672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:22:32,728 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_1.pt\n",
      "2025-11-08 05:22:32,729 - INFO - Epoch 1 completed. Average Loss: 0.8049\n",
      "2025-11-08 05:22:32,729 - INFO - Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial Final, Epoch 2/3, Batch 350/366. Current Loss: 0.8067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:30:56,690 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_2.pt\n",
      "2025-11-08 05:30:56,691 - INFO - Epoch 2 completed. Average Loss: 0.6761\n",
      "2025-11-08 05:30:56,691 - INFO - Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Trial Final, Epoch 3/3, Batch 350/366. Current Loss: 0.7726"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:39:20,442 - INFO - \n",
      "Checkpoint saved at model_checkpoints\\model_epoch_3.pt\n",
      "2025-11-08 05:39:20,443 - INFO - Epoch 3 completed. Average Loss: 0.6505\n",
      "2025-11-08 05:39:20,444 - INFO - Evaluating the model on the held-out TEST set.\n",
      "2025-11-08 05:40:02,063 - INFO - Accuracy: 71.18%\n",
      "2025-11-08 05:40:02,064 - INFO - F1 Score (Weighted): 71.17%\n",
      "2025-11-08 05:40:02,065 - INFO - \n",
      "Classification Report (Negative, Neutral, Positive):\n",
      "2025-11-08 05:40:02,076 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.78      0.78       835\n",
      "     Neutral       0.67      0.71      0.69      1277\n",
      "    Positive       0.70      0.67      0.69      1229\n",
      "\n",
      "    accuracy                           0.71      3341\n",
      "   macro avg       0.72      0.72      0.72      3341\n",
      "weighted avg       0.71      0.71      0.71      3341\n",
      "\n",
      "2025-11-08 05:40:02,077 - INFO - \n",
      "Confusion Matrix:\n",
      "2025-11-08 05:40:02,083 - INFO - [[655 104  76]\n",
      " [103 901 273]\n",
      " [ 76 331 822]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  Since our total revenue is below the ?10 crore limit, this part of the exemption is easily met.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "Probabilities: [0.014858348295092583, 0.1663081794977188, 0.8188334703445435]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  The immediate commencement means we had no grace period to prepare our internal systems for the new rules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Probabilities: [0.9568614363670349, 0.02660224959254265, 0.01653638482093811]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  We need to confirm which specific of the order matches with the final days of previous year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "Probabilities: [0.07507012039422989, 0.7830303907394409, 0.1418994963169098]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  The rule specifies the designated authority for the approval of scheme of arrangement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "Probabilities: [0.008391940034925938, 0.8229452967643738, 0.16866278648376465]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  The quarterly reporting requirement for all private companies is an excessive compliance burden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Probabilities: [0.9635326862335205, 0.0219976045191288, 0.014469759538769722]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  Introducing a simplified procedure for micro and small enterprises promotes ease of doing business.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "Probabilities: [0.006567909382283688, 0.10578414052724838, 0.8876479864120483]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  The amendment necessitates a revision of the company's existing Memorandum and Articles of Association.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "Probabilities: [0.02140389010310173, 0.6692243218421936, 0.3093717694282532]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter text for sentiment analysis (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Assuming all imports and global constants (NUM_LABELS, ID2LABEL, DATA_FILE_PATH, etc.)\n",
    "# are defined at the top of your script.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # CRITICAL FIX: FINAL 3-WAY DATA SPLITTING FOR TRAINING/TESTING\n",
    "    # ------------------------------------------------------------------------\n",
    "    logging.info(\"Preparing final Train/Test data sets...\")\n",
    "    df_full = pd.read_csv(DATA_FILE_PATH)\n",
    "    \n",
    "    # Data Cleaning and Mapping (Must match objective function logic)\n",
    "    columns_to_drop = ['Persona', 'Specific Clause Referenced', 'Unnamed: 4', 'Unnamed: 5']\n",
    "    df_clean = df_full.drop(columns=columns_to_drop, errors='ignore')\n",
    "    df_clean.rename(columns={'comment': 'text'}, inplace=True)\n",
    "    df_clean['label'] = df_clean['label'].map(LABEL2ID)\n",
    "\n",
    "    # A. Split into Main (80%) and Test (20%) - FINAL HELD-OUT TEST SET\n",
    "    df_main, df_test = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['label']) \n",
    "    \n",
    "    # B. Split Main (80%) into Train (70% total) and Validation (10% total) - FINAL TRAINING DATA\n",
    "    df_train, df_val = train_test_split(df_main, test_size=1/8, random_state=42, stratify=df_main['label'])\n",
    "    \n",
    "    # Convert to Hugging Face Dataset format\n",
    "    train_dataset = Dataset.from_pandas(df_train).remove_columns(['__index_level_0__'])\n",
    "    test_dataset = Dataset.from_pandas(df_test).remove_columns(['__index_level_0__'])\n",
    "    \n",
    "    # Initialize the Data Collator (CRITICAL for fixing batching errors)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    # The 'objective' function must contain the full 3-way split logic and evaluate on the VALIDATION set\n",
    "    study.optimize(objective, n_trials=10) \n",
    "\n",
    "    logging.info(\"Best hyperparameters:\")\n",
    "    logging.info(study.best_params)\n",
    "\n",
    "    # Load best hyperparameters\n",
    "    best_learning_rate = study.best_params[\"learning_rate\"]\n",
    "    best_batch_size = study.best_params[\"batch_size\"]\n",
    "\n",
    "    # --- FINAL TRAINING (Uses TRAIN SET + BEST HPs) ---\n",
    "    train_shuffled = train_dataset.shuffle(seed=42)\n",
    "    train_dataset = preprocess_data(tokenizer, train_shuffled)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=best_batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=data_collator # CRITICAL FIX\n",
    "    )\n",
    "\n",
    "    # --- FINAL TEST (Uses TEST SET for UNBIASED EVALUATION) ---\n",
    "    test_shuffled = test_dataset.shuffle(seed=42)\n",
    "    test_dataset = preprocess_data(tokenizer, test_shuffled)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=best_batch_size, \n",
    "        collate_fn=data_collator # CRITICAL FIX\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=best_learning_rate)\n",
    "    num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    logging.info(\"Starting final fine-tuning of the model.\")\n",
    "    # Pass 'Final' as the trial identifier for the status update\n",
    "    train_model(model, train_loader, optimizer, lr_scheduler, device, 'Final') \n",
    "\n",
    "    logging.info(\"Evaluating the model on the held-out TEST set.\")\n",
    "    predictions, labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # CRITICAL FIX 1: Use 'weighted' average for multi-class metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    logging.info(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # Logging the Weighted F1 Score is better for reporting\n",
    "    logging.info(f\"F1 Score (Weighted): {f1 * 100:.2f}%\")\n",
    "    \n",
    "    # CRITICAL FIX 2: Print the detailed Classification Report\n",
    "    logging.info(\"\\nClassification Report (Negative, Neutral, Positive):\")\n",
    "    logging.info(classification_report(labels, predictions, target_names=list(ID2LABEL.values()), zero_division=0))\n",
    "    \n",
    "    logging.info(\"\\nConfusion Matrix:\")\n",
    "    logging.info(confusion_matrix(labels, predictions))\n",
    "\n",
    "    # CRITICAL FIX 3: Correct 3-class sentiment mapping for interactive loop\n",
    "    while True:\n",
    "        text = input(\"\\nEnter text for sentiment analysis (or type 'exit' to quit): \")\n",
    "        if text.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        predicted_class, probabilities = classify_text(model, tokenizer, text, device)\n",
    "        # Use the global ID2LABEL dictionary to correctly map 0, 1, and 2\n",
    "        sentiment = ID2LABEL.get(predicted_class, \"Unknown\")\n",
    "\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa05bdc5-a0cf-4e19-b301-2c63f8076e87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configure logging (optional)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- CONFIGURATION (MUST MATCH TRAINING SETTINGS) ---\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "CHECKPOINT_DIR = \"model_checkpoints\"\n",
    "\n",
    "# CRITICAL: Path to your saved weights (adjust the epoch number if needed)\n",
    "LORA_WEIGHTS_PATH = os.path.join(CHECKPOINT_DIR, \"model_epoch_3.pt\")\n",
    "\n",
    "NUM_LABELS = 3\n",
    "ID2LABEL = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# --- 1. CLASSIFICATION FUNCTION (COPIED FROM YOUR CODE) ---\n",
    "def classify_text(model, tokenizer, text, device):\n",
    "    \"\"\"Tokenizes input text and returns the predicted class and probabilities.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "            probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
    "            \n",
    "            # Use the corrected 3-class mapping\n",
    "            sentiment = ID2LABEL.get(predicted_class, \"Unknown\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during text classification: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    return sentiment, predicted_class, probabilities\n",
    "\n",
    "# --- 2. MODEL LOADING AND INFERENCE SETUP ---\n",
    "def load_inference_model():\n",
    "    \"\"\"Loads the base model, applies LoRA configuration, and loads saved weights.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. Load Base Model Architecture (with 3-class head)\n",
    "    logging.info(\"Loading base model architecture...\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        id2label=ID2LABEL,\n",
    "        ignore_mismatched_sizes=True \n",
    "    )\n",
    "\n",
    "    # 2. Define LoRA Configuration (Must match training config)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
    "    )\n",
    "    \n",
    "    # 3. Apply the LoRA wrapper to the base model\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "    # 4. Load Saved Weights (Inference)\n",
    "    logging.info(f\"Loading saved LoRA weights from: {LORA_WEIGHTS_PATH}\")\n",
    "    if not os.path.exists(LORA_WEIGHTS_PATH):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {LORA_WEIGHTS_PATH}. Did you run the training script successfully?\")\n",
    "        \n",
    "    # CRITICAL: Load the saved LoRA weights onto the model\n",
    "    model.load_state_dict(torch.load(LORA_WEIGHTS_PATH, map_location=device))\n",
    "    \n",
    "    # 5. Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    logging.info(\"Model loaded and ready for inference.\")\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4457f71d-2f60-4945-b20b-23676581b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An unexpected error occurred: name 'load_inference_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- 3. MAIN INTERACTIVE INFERENCE BLOCK ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the trained model only once\n",
    "        model, tokenizer, device = load_inference_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"LoRA Sentiment Analyzer Ready. (Type 'exit' to quit)\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        while True:\n",
    "            text = input(\">> Enter comment: \")\n",
    "            if text.lower() == \"exit\":\n",
    "                break\n",
    "\n",
    "            sentiment, predicted_class_id, probabilities = classify_text(model, tokenizer, text, device)\n",
    "            \n",
    "            if sentiment:\n",
    "                print(f\"   [Prediction]: {sentiment}\")\n",
    "                print(f\"   [Probabilities]: Negative: {probabilities[0]:.4f}, Neutral: {probabilities[1]:.4f}, Positive: {probabilities[2]:.4f}\")\n",
    "            else:\n",
    "                print(\"   [Error]: Could not process comment.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFATAL ERROR: {e}\")\n",
    "        print(\"Please ensure your training script has successfully run and saved 'model_epoch_3.pt' in the 'model_checkpoints' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4a89d-b0d4-45e7-9f28-3a930a2d4236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
